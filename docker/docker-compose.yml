version: "3.0"
services:
    # postgres used by airflow
    postgres:
        image: postgres:9.6
        networks:
            - default_net
        volumes:
            # Create Test database on Postgresql
            - ./docker-airflow/pg-init-scripts:/docker-entrypoint-initdb.d
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        ports:
            - "5432:5432"

    # airflow LocalExecutor
    airflow-webserver:
        image: docker-airflow-spark:1.10.14_2.4.7
        restart: always
        networks:
            - default_net
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__CORE__FERNET_KEY=SrFOGUBJCOs5NMvZxJ0wA4KYXr7bOoueiBRvUe3WbFk=
            #v-STCavygWLUfWPZaSMvDwh0iIpMNxJddcvzq74Fz5c=
            - FERNET_KEY=SrFOGUBJCOs5NMvZxJ0wA4KYXr7bOoueiBRvUe3WbFk=
            - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
            #v-STCavygWLUfWPZaSMvDwh0iIpMNxJddcvzq74Fz5c=
            #- AIRFLOW_HOME=/usr/local/airflow
        volumes:
            - ../dags:/usr/local/airflow/dags #DAG folder
            - ../spark/app:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
            - ../spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
        ports:
            - "8282:8282"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3
        deploy:
            resources:
                limits:
                    cpus: '0.75'
                    memory: 3G
                reservations:
                    cpus: '0.25'
                    memory: 500M

    # Spark with 3 workers
    spark:
        image: bitnami/spark:2.4.6
        user: root # Run container as root container: https://docs.bitnami.com/tutorials/work-with-non-root-containers/
        hostname: spark
        networks:
            - default_net
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - "SPARK_MASTER_WEBUI_PORT=8080"
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ../spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
            - ../spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
        deploy:
            resources:
                limits:
                    cpus: '1'
                    memory: 3G
                reservations:
                    cpus: '0.25'
                    memory: 500M
        ports:
            - "8080:8080"
            - "7077:7077"

    spark-worker-1:
        image: bitnami/spark:2.4.6
        user: root
        depends_on:
            - spark
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_WEBUI_PORT=8082
            - SPARK_WORKER_MEMORY=2G
            - SPARK_WORKER_CORES=2
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ../spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
            - ../spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
        ports:
            - 8082:8082
        deploy:
            resources:
                limits:
                    cpus: '1'
                    memory: 2G
                reservations:
                    cpus: '0.25'
                    memory: 500M
    spark-worker-2:
        image: bitnami/spark:2.4.6
        user: root
        depends_on:
            - spark
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_WEBUI_PORT=8081
            - SPARK_WORKER_MEMORY=2G
            - SPARK_WORKER_CORES=2
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        ports:
            - 8081:8081
        volumes:
            - ../spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
            - ../spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
        deploy:
            resources:
                limits:
                    cpus: '1'
                    memory: 2G
                reservations:
                    cpus: '0.25'
                    memory: 500M
    spark-worker-3:
        image: bitnami/spark:2.4.6
        user: root
        depends_on:
            - spark
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_WEBUI_PORT=8181
            - SPARK_WORKER_MEMORY=2G
            - SPARK_WORKER_CORES=2
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        ports:
            - 8181:8181
        volumes:
            - ../spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
            - ../spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
        deploy:
            resources:
                limits:
                    cpus: '1'
                    memory: 2G
                reservations:
                    cpus: '0.25'
                    memory: 500M
    spark-worker-4:
        image: bitnami/spark:2.4.6
        user: root
        depends_on:
            - spark
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_WEBUI_PORT=8180
            - SPARK_WORKER_MEMORY=2G
            - SPARK_WORKER_CORES=2
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        ports:
            - 8180:8180
        volumes:
            - ../spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
            - ../spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
        deploy:
            resources:
                limits:
                    cpus: '1'
                    memory: 2G
                reservations:
                    cpus: '0.25'
                    memory: 500M
#    db:
#
#        image: mysql:8.0
#        container_name: spark_mysql
#        command: '--default-authentication-plugin=mysql_native_password'
#        ports:
#            - "32001:3306"
#        restart: always
#        environment:
#            MYSQL_USER: miadmin
#            MYSQL_PASSWORD: Mi4man11
#            MYSQL_ROOT_PASSWORD: Mi4man11
#            MYSQL_DATABASE: covid_economy_impact
#        networks:
#            - default_net
#        volumes:
#            - /Users/syeruvala/Exercism/python/capstone_final_pipeline/spark_airflow_data_pipeline/docker/mysql-docker/mysql-initdb.d:/docker-entrypoint-initdb.d
#            - /Users/syeruvala/Exercism/python/capstone_final_pipeline/spark_airflow_data_pipeline/docker/mysql_data:/var/lib/mysql
    jupyter-spark:
        image: jupyter/pyspark-notebook
        networks:
            - default_net
        ports:
            - "8888:8888"
            - "4040-4080:4040-4080"
        volumes:
            - ../notebooks:/home/jovyan/work/notebooks/
            - ../spark/resources/data:/home/jovyan/work/data/


networks:
    default_net:
